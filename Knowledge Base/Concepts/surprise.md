it's inversely proportional to probability.
(Example: Predicting a coin flip correctly is not that surprising, because the chances are 50-50. Predicting something 100% certain is not surprising at all (100% certainty - 0% surpirse). Predicting the lottery numbers has close to 0% certainty - 100% surprise)

Where probabilities multiply, the surprise should add. The [[logarithm]] works this way.

h(s) = log(1/ps), where
h(s): surprise function (takes in a state s as an input)
ps: probability of state s occurring.

The average surprise is what we call the [[entropy]] of the distribution.